SUMMARY
===============

A music streaming startup, *** Sparkify***, has expanded their user base and song database and want to move their processes and data onto the cloud. Their data is stored in S3, in a directory of JSON logs of user activity, as well as a directory with JSON metadata of the songs in their app. 
So, as a data engineer my job was to build an ETL pipeline to extract data from S3, load them in staging tables in redshift and then transform the data into a set of analytics tables. This would help the analytics team to continue finding insights in less time from a huge amount of data.

Designing database schema and creating an ETL pipeline.
===============

In the this datawarehouse project, Database schema was designed in such a way that the analysis as well as data retrieval part become easy for the analytics team of sparkify. I created 5 analytics tables with relevant table optimization techniques.

***Steps went into creating an ETL pipeline were:***

1- Created a redshift cluster on AWS and use its endpoint to create a managed connection between Redshift Cluster and Amazon S3.
2- Created a IAM role to tell redshift to obtain its data from S3.
3- Made a connection to the database associated with redshift.
4- Extract data from S3 and load them into staging tabels in redshift.
5- Transform data stored in staging tabels into analytical tabels. (4 dimension tabels and 1 fact tabel).
6- Close the connection to the database.


Running Python Scripts
==========================

In order to test the extraction of data from S3, loading of data in staging tabels and transformation of data into analytics tabels, three python scripts have been created and run using terminal command 'python _script_name_.py'.
Their are three python scripts:
1- sql_queries.py
2- etl.py
3- create_tables.py

- __create_tables.py__: In order to extract data from s3 and stage them in redshift, we first have to create staging tables as well dimension tables. So, for that we  first run create_tables.py script. This script will establish connection to the redshift database and create tables in it using create table queries. 

- __etl.py__: After the creation of tabels, we have to load data in staging and analytics tabels respectively by running etl.py script. This python script will again make connection to the database in redshift and start loading data in staging tables from S3. Also, this script will insert data from staging tables to four dimension tables and one fact table.

- __dwh.cfg__: This file contains all the credentials which are require to make connection to S3 and redshift cluster.

Files in the repository
===========================

Their are basically two datasets which resides in S3:
1- __Song__ dataset: This data is a subset of real data from [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Data is present in json format and contains metadata about song and its artist. The files are partitioned by the first three letters of each song's track ID. For example:
- _song_data/A/B/C/TRABCEI128F424C983.json: A/B/C track ID._
- _song_data/A/A/B/TRAABJL12903CDCF1A.json_: A/A/B track ID._

Here is an example of one json file song data.

_{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}_

2- __Log__ dataset: This dataset is in json format generated by the [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.
The log files in the dataset are partitioned by year and month. For example:
- log_data/2018/11/2018-11-12-events.json
- log_data/2018/11/2018-11-13-events.json

Example Queries
===============

1- __Creating staging events table.__

_staging_events_table_create= ("""
                              CREATE TABLE IF NOT EXISTS stage_events
                              (artist varchar(500),
                              auth varchar(50),
                              firstName varchar(50),
                              gender varchar(6),
                              itemInSession int,
                              lastName varchar(50),
                              length real,
                              level varchar(50),
                              location varchar(500),
                              method varchar,
                              page varchar,
                              registration real,
                              session_id smallint,
                              song varchar,
                              status smallint,
                              ts bigint,
                              userAgent varchar(500),
                              userId smallint);  
                              """)_

2- __Creating songplays (fact) table.__

_songplay_table_create = ("""
                        CREATE TABLE IF NOT EXISTS songplays
                        (songplay_id int IDENTITY(0,1) NOT NULL PRIMARY KEY SORTKEY,
                        start_time timestamp NOT NULL REFERENCES time(start_time), 
                        user_id smallint NOT NULL REFERENCES users(user_id) DISTKEY, 
                        level varchar(500), 
                        song_id varchar(500) NOT NULL REFERENCES songs(song_id),
                        artist_id varchar(500) NOT NULL REFERENCES artists(artist_id), 
                        session_id smallint, 
                        location varchar(500), 
                        user_agent varchar(500));
                        """)_
                        

3- __Queries to insert data into fact table songplays.__


_songplay_table_insert = ("""
                         INSERT INTO songplays (start_time, user_id, level, song_id,
                         artist_id, session_id, location, user_agent)                          
                         SELECT
                         to_timestamp(stage_events.ts::text, 'YYYYMMDDHH24MISS') AS start_time,
                         stage_events.userId AS user_id,
                         stage_events.level,
                         stage_songs.song_id,
                         stage_songs.artist_id,
                         stage_events.session_id,
                         stage_events.location,
                         stage_events.userAgent AS user_agent
                         FROM stage_events
                         JOIN stage_songs ON (stage_events.artist = stage_songs.artist_name 
                         AND stage_events.song = stage_songs.title)
                         WHERE stage_events.page = 'NextSong'
                         AND stage_events.userId NOT IN (SELECT DISTINCT user_id FROM songplays);
                         """)_

